{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_truth(list_series):\n",
    "    data = []\n",
    "    for subject in xrange(1,13):\n",
    "        for se in list_series:\n",
    "            file_name = 'data/train/subj' + str(subject) + \"_series\" + str(se)\n",
    "\n",
    "            evt = pd.read_csv(file_name + '_events.csv')\n",
    "            evt.drop(\"id\", axis = 1, inplace=True)\n",
    "        \n",
    "            data.append( evt )\n",
    " \n",
    "    return pd.concat( data, ignore_index = True ).values.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv('predictions_8.csv')\n",
    "ids = preds['id'].values\n",
    "preds.drop(\"id\", axis = 1, inplace=True)\n",
    "preds = preds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truth = load_truth([8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uncalibrated roc auc score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87103770233858147"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score( truth , preds, average = 'micro' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subjects = np.zeros( ids.shape[0] )\n",
    "for i, id_str in enumerate(ids):\n",
    "    subjects[i] = int( id_str[4: id_str.find('_') ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* roc auc score averaging over subjects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject 1 score: 0.960404771891\n",
      "subject 2 score: 0.897414923255\n",
      "subject 3 score: 0.841515547073\n",
      "subject 4 score: 0.854365441438\n",
      "subject 5 score: 0.803649202557\n",
      "subject 6 score: 0.897101764226\n",
      "subject 7 score: 0.864116496382\n",
      "subject 8 score: 0.897644238315\n",
      "subject 9 score: 0.856652370583\n",
      "subject 10 score: 0.912150571027\n",
      "subject 11 score: 0.866506379981\n",
      "subject 12 score: 0.811814455157\n",
      "\n",
      " Mean score: 0.871944680157\n"
     ]
    }
   ],
   "source": [
    "mean_score = 0\n",
    "for subject in xrange(1,13):\n",
    "    mask = (subjects == subject)\n",
    "    truth_subject = truth[mask]\n",
    "    preds_subject = preds[mask]\n",
    "    \n",
    "    score = roc_auc_score( truth_subject , preds_subject, average = 'micro' )\n",
    "    print 'subject', subject, 'score:', score\n",
    "    mean_score += score\n",
    "    \n",
    "print '\\n Mean score:', mean_score/12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uncalibrated score is close to the per subject mean one: no need to calibrate, which is nice since it's neither very interesting nor close to what we want to do in real life."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
